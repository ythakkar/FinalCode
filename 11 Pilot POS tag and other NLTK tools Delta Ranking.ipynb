{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## libraries\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import scipy.spatial.distance\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## data loading\n",
    "qa = np.genfromtxt(\"uid_qa.txt\", delimiter = \",\", names = True, dtype = [('int64'), ('int64'), ('int64'), ('U256'), ('U128'),])\n",
    "fe = np.genfromtxt(\"uid_pre_elim.txt\", delimiter = \",\", names = True, dtype = [('int64'), ('int64'), ('int64'), ('U256'), ('U256'),])\n",
    "ftd = np.genfromtxt(\"face_id_descr.txt\", delimiter = \";\", skip_header = 1 , usecols = np.arange(0,2), dtype = [('U16'), ('U2056')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#changing or adding names for the dtypes\n",
    "fe.dtype.names = ('uniqueID', 'bn', 'qn', 'pre_que', 'curr_elim')\n",
    "ftd.dtype.names = ('img_id', 'description')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## first convert strings to list of rows equal to rows in fe\n",
    "def mk_list(str_vector):\n",
    "    des_byid = str_vector\n",
    "    all_x = []\n",
    "    for i in des_byid:\n",
    "        j = i.split()\n",
    "        #if len(j) > 0:\n",
    "        all_x.append(j)\n",
    "\n",
    "    return all_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_x = mk_list(fe['pre_que'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##check blanks\n",
    "## As there are some rows where there is only one image or blank/erronous question. We check that and save those indices.\n",
    "## create blank list\n",
    "#good_index = ['True']*len(all_x)\n",
    "\n",
    "bl =[]\n",
    "\n",
    "for img,q in zip(enumerate(all_x), mk_list(qa['que'])):\n",
    "    if (len(img[1]) <2 or len(q) < 2) and img[0] not in bl:\n",
    "        bl.append(img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Now for each row in the list above, find description and add it\n",
    "all_i_txt1 = []\n",
    "original_ind = []\n",
    "bl_ind = []\n",
    "\n",
    "for rown in range(len(all_x)):\n",
    "    if rown in bl:\n",
    "        bl_ind.append(rown)\n",
    "    else:\n",
    "        #print(all_x.index)\n",
    "        int_i_txt = []\n",
    "        for img in all_x[rown]:\n",
    "            if img in ftd['img_id']:\n",
    "                d = str(ftd['description'][list(ftd['img_id']).index(img)])\n",
    "                int_i_txt.append(str(d))\n",
    "        original_ind.append(rown)\n",
    "        all_i_txt1.append(int_i_txt)\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## For each question, find the synonyms for all the adjectives, NN, NNS and CD. Then add that list of synonyms in the \n",
    "##column called question \n",
    "\n",
    "new_table_fd = [] ## for frequency distribution\n",
    "new_que_syn = []\n",
    "for rown in qa['que'] :\n",
    "    #print(rown[-2])\n",
    "    #new_row = []\n",
    "    sents = nltk.sent_tokenize(rown.strip())\n",
    "    \n",
    "    lw = [nltk.word_tokenize(s) for s in sents]\n",
    "    #p =[nltk.pos_tag(w) for w in lw]\n",
    "    x = [kv[0] for w in lw for kv in nltk.pos_tag(w) if kv[1] in ['JJ', 'NN', 'NNS', 'CD']]\n",
    "    fd = [kv[1] for w in lw for kv in nltk.pos_tag(w)]\n",
    "    #y = [wordnet.synsets(words) for words in x]\n",
    "    syn = [j.name() for words in x for i in wordnet.synsets(words) for j in i.lemmas()]\n",
    "    new_que_syn.append(list(set(syn)))\n",
    "    new_table_fd.extend([kv[0] for w in lw for kv in nltk.pos_tag(w)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_i_txt = [row + [' '.join(map(str, new_que_syn[rn])), qa['ans'][rn].strip()] for rn, row in zip(original_ind, all_i_txt1)]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(all_i_txt[0])\n",
    "##basic model\n",
    "tf = TfidfVectorizer(ngram_range = (2,4), sublinear_tf = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_sim_tf = []\n",
    "for l in range(len(all_i_txt)):\n",
    "    H = tf.fit_transform(all_i_txt[l])\n",
    "    sim = cosine_similarity(H)\n",
    "    all_sim_tf.append(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#similarities[-2] meaning taking only the array for question\n",
    "all_sim_q_tf = []\n",
    "for i in range(len(all_sim_tf)):\n",
    "    all_sim_q_tf.append(all_sim_tf[i][-2][:-2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Sort all the above similarities with argsort which gives indices\n",
    "## or we should not take top5 and make a threshold above which we will include it in the list of output?\n",
    "sorted_ind_tf = []\n",
    "for si in all_sim_q_tf:\n",
    "    y_top = si.argsort()[::-1]\n",
    "    sorted_ind_tf.append(y_top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_sim_all_tf = []\n",
    "for q,w in zip(sorted_ind_tf, all_sim_q_tf):\n",
    "    sorted_sim = []\n",
    "    for e in q:\n",
    "        sorted_sim.append(w[e])\n",
    "    sorted_sim_all_tf.append(sorted_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##find the difference between two similarity score in each array.\n",
    "sim_dif_all_tf = []\n",
    "\n",
    "for d in range(len(sorted_sim_all_tf)):\n",
    "    sim_dif = [sorted_sim_all_tf[d][f-1] - sorted_sim_all_tf[d][f] for f in range(1, len(sorted_sim_all_tf[d]))]\n",
    "    sim_dif_all_tf.append(sim_dif)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "max_dif_tf = []\n",
    "max_dif_ind_tf =[]\n",
    "for g in range(len(sim_dif_all_tf)):\n",
    "    \n",
    "    maxi= max(sim_dif_all_tf[g])\n",
    "    max_dif_tf.append(maxi)\n",
    "    max_dif_ind_tf.append(sim_dif_all_tf[g].index(maxi))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_ind_lim_tf = [indi[0:max_indi+1] for indi,max_indi in zip(sorted_ind_tf, max_dif_ind_tf)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##clean_all_x\n",
    "all_x_clean_tf = [all_x[i] for i in original_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## convert to img_id from index to match with actual image ids selcted by participant\n",
    "def mk_imgid(arr):\n",
    "    y_pred = []\n",
    "    for i,j in arr:\n",
    "        yp = []\n",
    "        for s in j:\n",
    "            yp.append(i[s])\n",
    "        y_pred.append(yp)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_img_tf = mk_imgid(zip(all_x_clean_tf, sorted_ind_lim_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_cheat1 = mk_list(fe['curr_elim'])\n",
    "all_cheat_tf = [all_cheat1[i] for i in original_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pre_all= [len(set(a).intersection(set(p)))/len(set(p)) if len(p) > 0 else 0 for a,p in zip(all_cheat_tf, pred_img_tf)]\n",
    "rec_all = [len(set(a).intersection(set(p)))/len(set(a)) if len(a) > 0 else 0 for a,p in zip(all_cheat_tf, pred_img_tf)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fs_all= [(0.5*0.5 + 1)*pr*re/ (0.5*0.5*pr + re) if pr > 0 or re >0 else 0 for pr, re in zip(pre_all, rec_all)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pre_mean = np.mean(np.array(pre_all))\n",
    "rec_mean = np.mean(np.array(rec_all))\n",
    "fs_mean = np.mean(np.array(fs_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with open('Results with description.txt', 'w') as f:\n",
    "#    f.write(\"No., Model, Parameter, Decision Rules, Precision, Recall, F0.5\"+ '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "op3 = [\"60\", \"TFIDF, NLTK, POS\", \"ngram 2,4 sublinear\", \"Cosinesimilarity and ranking difference\",str(pre_mean), str(rec_mean), str(fs_mean)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('Results with description.txt', 'a') as f:\n",
    "    f.write('; '.join(op3)+ '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Countvectorizr starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range = (2,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## all similarities\n",
    "all_sim_cv = []\n",
    "for l in range(len(all_i_txt)):\n",
    "    H = cv.fit_transform(all_i_txt[l])\n",
    "    sim = cosine_similarity(H)\n",
    "    all_sim_cv.append(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#similarities[-2] meaning taking only the array for question\n",
    "all_sim_q_cv = []\n",
    "for i in range(len(all_sim_cv)):\n",
    "    all_sim_q_cv.append(all_sim_cv[i][-2][:-2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Sort all the above similarities with argsort which gives indices\n",
    "## or we should not take top5 and make a threshold above which we will include it in the list of output?\n",
    "sorted_ind_cv = []\n",
    "for si in all_sim_q_cv:\n",
    "    y_top = si.argsort()[::-1]\n",
    "    sorted_ind_cv.append(y_top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_sim_all_cv = []\n",
    "for q,w in zip(sorted_ind_cv, all_sim_q_cv):\n",
    "    sorted_sim = []\n",
    "    for e in q:\n",
    "        sorted_sim.append(w[e])\n",
    "    sorted_sim_all_cv.append(sorted_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##find the difference between two similarity score in each array.\n",
    "sim_dif_all_cv = []\n",
    "\n",
    "for d in range(len(sorted_sim_all_cv)):\n",
    "    sim_dif = [sorted_sim_all_cv[d][f-1] - sorted_sim_all_cv[d][f] for f in range(1, len(sorted_sim_all_cv[d]))]\n",
    "    sim_dif_all_cv.append(sim_dif)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "max_dif_cv = []\n",
    "max_dif_ind_cv =[]\n",
    "for g in range(len(sim_dif_all_cv)):\n",
    "    \n",
    "    maxi= max(sim_dif_all_cv[g])\n",
    "    max_dif_cv.append(maxi)\n",
    "    max_dif_ind_cv.append(sim_dif_all_cv[g].index(maxi))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_ind_lim_cv = [indi[0:max_indi+1] for indi,max_indi in zip(sorted_ind_cv, max_dif_ind_cv)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_x_clean_cv = [all_x[i] for i in original_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_img_cv = mk_imgid(zip(all_x_clean_cv, sorted_ind_lim_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_cheat1 = mk_list(fe['curr_elim'])\n",
    "all_cheat_cv = [all_cheat1[i] for i in original_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pre_all_cv= [len(set(a).intersection(set(p)))/len(set(p)) if len(p) > 0 else 0 for a,p in zip(all_cheat_cv, pred_img_cv)]\n",
    "rec_all_cv = [len(set(a).intersection(set(p)))/len(set(a)) if len(a) > 0 else 0 for a,p in zip(all_cheat_cv, pred_img_cv)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fs_all_cv= [(0.5*0.5 + 1)*pr*re/ (0.5*0.5*pr + re) if pr > 0 or re >0 else 0 for pr, re in zip(pre_all_cv, rec_all_cv)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pre_mean_cv = np.mean(np.array(pre_all_cv))\n",
    "rec_mean_cv = np.mean(np.array(rec_all_cv))\n",
    "fs_mean_cv = np.mean(np.array(fs_all_cv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "op3 = [\"64\", \"cv, NLTK, POS\", \"ngram_range 2,5\", \"Cosinesimilarity and ranking difference\",str(pre_mean_cv), str(rec_mean_cv), str(fs_mean_cv)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('Results with description.txt', 'a') as f:\n",
    "    f.write('; '.join(op3)+ '\\n')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
